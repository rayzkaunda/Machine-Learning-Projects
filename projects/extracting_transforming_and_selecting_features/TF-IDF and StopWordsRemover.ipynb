{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### TF-IDF\n",
    "Term frequency-inverse document frequency (TF-IDF) is a feature vectorization method widely used in text mining to reflect the importance of a term to a document in the corpus. Denote a term by t, a document by d, and the corpus by D. Term frequency TF(t,d) is the number of times that term t appears in document d, while document frequency DF(t,D) is the number of documents that contains term t. If we only use term frequency to measure the importance, it is very easy to over-emphasize terms that appear very often but carry little information about the document, e.g. “a”, “the”, and “of”. If a term appears very often across the corpus, it means it doesn’t carry special information about a particular document. Inverse document frequency is a numerical measure of how much information a term provides.\n",
    "\n",
    "In Spark you can use the "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.feature import HashingTF, IDF, Tokenizer\n",
    "\n",
    "#create a dataframe \n",
    "sentenceData = spark.createDataFrame([\n",
    "    (0.0, \"Hi I heard today is National Bacon Day!\"),\n",
    "    (0.0, \"I want bacon on everything\"),\n",
    "    (1.0, \"Spark is like bacon on pizza!\")\n",
    "], [\"label\", \"sentence\"])\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+--------------------+\n",
      "|label|            sentence|\n",
      "+-----+--------------------+\n",
      "|  0.0|Hi I heard today ...|\n",
      "|  0.0|I want bacon on e...|\n",
      "|  1.0|Spark is like bac...|\n",
      "+-----+--------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "sentenceData.show() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+--------------------+--------------------+\n",
      "|label|            sentence|               words|\n",
      "+-----+--------------------+--------------------+\n",
      "|  0.0|Hi I heard today ...|[hi, i, heard, to...|\n",
      "|  0.0|I want bacon on e...|[i, want, bacon, ...|\n",
      "|  1.0|Spark is like bac...|[spark, is, like,...|\n",
      "+-----+--------------------+--------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#next we use the tokenizer to break sentences into works\n",
    "tokenizer = Tokenizer(inputCol=\"sentence\", outputCol=\"words\")\n",
    "wordsData = tokenizer.transform(sentenceData)\n",
    "wordsData.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+--------------------+--------------------+--------------------+\n",
      "|label|            sentence|               words|         rawFeatures|\n",
      "+-----+--------------------+--------------------+--------------------+\n",
      "|  0.0|Hi I heard today ...|[hi, i, heard, to...|(20,[0,1,2,5,9,10...|\n",
      "|  0.0|I want bacon on e...|[i, want, bacon, ...|(20,[2,5,9,12,16]...|\n",
      "|  1.0|Spark is like bac...|[spark, is, like,...|(20,[1,2,5,10,13,...|\n",
      "+-----+--------------------+--------------------+--------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#next we apply the words column on the hashingTF transformer into a vector of features\n",
    "hashingTF = HashingTF(inputCol=\"words\", outputCol=\"rawFeatures\", numFeatures=20)\n",
    "featurizedData = hashingTF.transform(wordsData)\n",
    "featurizedData.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+--------------------+\n",
      "|label|            features|\n",
      "+-----+--------------------+\n",
      "|  0.0|(20,[0,5,9,17],[0...|\n",
      "|  0.0|(20,[2,7,9,13,15]...|\n",
      "|  1.0|(20,[4,6,13,15,18...|\n",
      "+-----+--------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# alternatively, CountVectorizer can also be used to get term frequency vectors\n",
    "idf = IDF(inputCol=\"rawFeatures\", outputCol=\"features\")\n",
    "idfModel = idf.fit(featurizedData)\n",
    "rescaledData = idfModel.transform(featurizedData)\n",
    "\n",
    "rescaledData.select(\"label\", \"features\").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Stop Words Remover\n",
    "Stop words are words which should be excluded from the input, typically because the words appear frequently and don’t carry as much meaning.\n",
    "\n",
    "StopWordsRemover takes as input a sequence of strings (e.g. the output of a Tokenizer) and drops all the stop words from the input sequences. The list of stopwords is specified by the stopWords parameter. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+----------------------------+--------------------+\n",
      "|id |raw                         |filtered            |\n",
      "+---+----------------------------+--------------------+\n",
      "|0  |[I, saw, the, red, balloon] |[saw, red, balloon] |\n",
      "|1  |[Mary, had, a, little, lamb]|[Mary, little, lamb]|\n",
      "+---+----------------------------+--------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "from pyspark.ml.feature import StopWordsRemover\n",
    "\n",
    "sentenceData = spark.createDataFrame([\n",
    "    (0, [\"I\", \"saw\", \"the\", \"red\", \"balloon\"]),\n",
    "    (1, [\"Mary\", \"had\", \"a\", \"little\", \"lamb\"])\n",
    "], [\"id\", \"raw\"])\n",
    "\n",
    "remover = StopWordsRemover(inputCol=\"raw\", outputCol=\"filtered\")\n",
    "remover.transform(sentenceData).show(truncate=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
